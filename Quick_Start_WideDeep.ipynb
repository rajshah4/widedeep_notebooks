{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Getting Started with Pytorch WideDeep\n",
    "\n",
    "`pytorch-widedeep` is a flexible and modular way to build deep learning models. You can start with multimodal data (wide) while using deep learning to find complex relationships in your data (deep).  An example use case is predicting the value of a house based on images of the house, tabular data (e.g., number of rooms, floor area), and a text data (e.g, a detailed description).  With `pytorch-widedeep` you can bring all those disparate types of data into one deep learning model!\n",
    "\n",
    "### Is this package right for you?\n",
    "\n",
    "This notebook focuses on a deep learning approach to tabular data.  If you are just getting started, you are probably better off using an approach like gradient boosted machines, which typically perform better on tabular data.  Check out [Javier's post](https://jrzaurin.github.io/infinitoml/2021/05/28/pytorch-widedeep_iv.html) for a comparison of GBM versus deep learning.  However, for people with larger datasets, more complex features, multiclass, multitarget, or multitask, using a deep learning approach has benefits, check out how [Pinterest](https://medium.com/pinterest-engineering/how-we-use-automl-multi-task-learning-and-multi-tower-models-for-pinterest-ads-db966c3dc99e) or [Lyft](https://twimlai.com/causal-models-in-practice-at-lyft-with-sean-taylor/) have moved to deep learning models.\n",
    "\n",
    "The notebook focuses on `pytorch-widedeep` which I like, but the pytorch ecoysystem has lots of other great packages I would recommend including [fast.ai](https://docs.fast.ai/), [pytorch-tabular](https://pytorch-tabular.readthedocs.io/en/latest/), and [pytorch-forecasting](https://pytorch-forecasting.readthedocs.io/en/latest/).    \n",
    "\n",
    "### How to use this notebook\n",
    "\n",
    "The notebook is structured to walk you through a typical workflow of preprocessing your data, defining a model, training a model, and then getting predictions. We start with a simple MLP with TabMLP and then move to more complex approaches like the TabResnet, and TabTransformer. Links to additional documentation and academic papers are also included.\n",
    "\n",
    "The notebook is intended to be an interactive way to explore the pytorch models. Take time to look at each object that is created and understand what is happening at every step. This notebook is not intended as boilerplate code for building a production mode. Finally, this notebook is just the start. There are plenty of other [example notebooks](https://github.com/jrzaurin/pytorch-widedeep/tree/master/examples) that cover more advanced uses like [combining images and text](https://github.com/jrzaurin/pytorch-widedeep/blob/master/examples/05_Regression_with_Images_and_Text.ipynb). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install and Data Prep"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Click here to open in Google Colab](https://colab.research.google.com/github/rajshah4/widedeep_notebooks/blob/main/Quick_Start_WideDeep.ipynb).  \n",
    "\n",
    "Installing pytorch-widedeep is a an important first step. After, that we will import in the necessary libraries for this notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "##Packages to install when using Google Colab\n",
    "#!pip install pytorch-widedeep\n",
    "#!pip install captum\n",
    "#You should then restart the runtime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.training import Trainer\n",
    "from pytorch_widedeep.models import Wide, TabMlp, TabResnet, TabTransformer, WideDeep, TabFastFormer, TabPerceiver\n",
    "from pytorch_widedeep.metrics import Accuracy, Precision\n",
    "\n",
    "from captum.attr._core.feature_permutation import FeaturePermutation\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/rajiv.shah/anaconda3/envs/pytorch/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset for this notebooks is the [UCI Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) dataset. This dataset was chosen because it has a mix of numerical and categorical features.  Let's prepare our data by putting all of the features and our target into one pandas dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
    "names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', \n",
    "                 'marital_status', 'occupation', 'relationship', 'race', 'sex', \n",
    "                 'capital_gain', 'capital_loss', 'hours_per_week', \n",
    "                 'native_country', 'income']\n",
    "\n",
    "df = pd.read_csv('adult.data', header=None, names=names, na_values=['?', ' ?'])\n",
    "#create a binary target\n",
    "df['income_label'] = (df[\"income\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "df.drop('income', axis=1, inplace=True)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-09-28 17:27:10--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3974305 (3.8M) [application/x-httpd-php]\n",
      "Saving to: ‘adult.data.18’\n",
      "\n",
      "adult.data.18       100%[===================>]   3.79M  5.28MB/s    in 0.7s    \n",
      "\n",
      "2021-09-28 17:27:11 (5.28 MB/s) - ‘adult.data.18’ saved [3974305/3974305]\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education_num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital_status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country  income_label  \n",
       "0          2174             0              40   United-States             0  \n",
       "1             0             0              13   United-States             0  \n",
       "2             0             0              40   United-States             0  \n",
       "3             0             0              40   United-States             0  \n",
       "4             0             0              40            Cuba             0  "
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing the data\n",
    "\n",
    "For data with high cardinality categoricals, text, image data  . .. or anything that is wide, we applying some preprocessing.  The simplest approach `Wide` is shown below for categoricals.  Here we implement a liner layer as an Embedding layer plus a bias.  When working with text use `DeepText` or with images use `DeepImage`.  There is also `DeepTabular` when you want to build XYZ on tabular datasets.  The examples section includes more detailed notebooks on [preprocessing](https://github.com/jrzaurin/pytorch-widedeep/blob/master/examples/01_Preprocessors_and_utils.ipynb).\n",
    "\n",
    "Here we define the categoricval features that should be treated as wide.  We also perform additional feature engineering by indicating that `education` and `occupation` are crossed columns.  This will manually create interaction features between the two features.  The WidePreprocessor label encodes all the categorical features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "wide_cols = ['education', 'relationship','workclass','occupation','native_country','sex']\n",
    "crossed_cols = [('education', 'occupation'), ('native_country', 'occupation')]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "wide_preprocessor = WidePreprocessor(wide_cols=wide_cols, crossed_cols=crossed_cols)\n",
    "X_wide = wide_preprocessor.fit_transform(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For `DeepTabular`, we define columns with categorical features that should create embeddings with `cat_embed_cols` and features that are continious numerical with `continuous_cols`.  The resulting dataset gets label-encoded for the categorical columns and normalizes the numerical ones."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "cat_embed_cols = [('education',16), ('relationship',8), ('workclass',16), ('occupation',16),('native_country',16)]\n",
    "continuous_cols = [\"age\",\"hours_per_week\"]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "tab_preprocessor = TabPreprocessor(embed_cols=cat_embed_cols, continuous_cols=continuous_cols)\n",
    "X_tab = tab_preprocessor.fit_transform(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# TARGET\n",
    "target_col = 'income_label'\n",
    "target = df[target_col].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling Setup\n",
    "\n",
    "Modeling requires bringing together the preprocessed components and applying one of the deep tabular models.  There is considerable flexibility in the library for how you are doing this.  There is everything from the very simple `TabMlp` to the well-known `TabNet` to more cutting edge approaches like `Perceiver`, and `FastFormer`.  To see more examples, check out the notebook that focuses on [deeptabular models](https://github.com/jrzaurin/pytorch-widedeep/blob/master/examples/02_2_deeptabular_models.ipynb) or the [transformer models](https://github.com/jrzaurin/pytorch-widedeep/blob/master/examples/10_The_Transformer_Family.ipynb).\n",
    "\n",
    "For this notebook, we start by taking our wide preprocessed dataset and putting it through a very simple MLP that represents the categorical features and concatenates the continuous features.  You can view the model object to see the structure of the model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "deeptabular = TabMlp(mlp_hidden_dims=[64,32], \n",
    "                      column_idx=tab_preprocessor.column_idx,\n",
    "                      embed_input=tab_preprocessor.embeddings_input,\n",
    "                      continuous_cols=continuous_cols)\n",
    "model = WideDeep(wide=wide, deeptabular=deeptabular)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "Once the model is built, we just need to compile it and run it.  The Trainer is where you would look for the typical options for training a model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "mlp_model = Trainer(model, objective='binary', metrics=[Accuracy, Precision])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "mlp_model.fit(X_wide=X_wide, X_tab=X_tab, target=target, n_epochs=5, batch_size=64, val_split=0.2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/rajiv.shah/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "epoch 1: 100%|██████████| 407/407 [00:03<00:00, 126.41it/s, loss=0.447, metrics={'acc': 0.7926, 'prec': 0.5889}]\n",
      "valid: 100%|██████████| 102/102 [00:00<00:00, 112.72it/s, loss=0.37, metrics={'acc': 0.8165, 'prec': 0.6522}]\n",
      "epoch 2: 100%|██████████| 407/407 [00:02<00:00, 140.56it/s, loss=0.386, metrics={'acc': 0.8184, 'prec': 0.6538}]\n",
      "valid: 100%|██████████| 102/102 [00:00<00:00, 212.98it/s, loss=0.36, metrics={'acc': 0.8237, 'prec': 0.6598}]\n",
      "epoch 3: 100%|██████████| 407/407 [00:03<00:00, 127.79it/s, loss=0.375, metrics={'acc': 0.8247, 'prec': 0.6689}]\n",
      "valid: 100%|██████████| 102/102 [00:00<00:00, 209.80it/s, loss=0.357, metrics={'acc': 0.8253, 'prec': 0.6789}]\n",
      "epoch 4: 100%|██████████| 407/407 [00:02<00:00, 140.71it/s, loss=0.367, metrics={'acc': 0.8269, 'prec': 0.6751}]\n",
      "valid: 100%|██████████| 102/102 [00:00<00:00, 215.99it/s, loss=0.353, metrics={'acc': 0.8311, 'prec': 0.6701}]\n",
      "epoch 5: 100%|██████████| 407/407 [00:02<00:00, 149.23it/s, loss=0.358, metrics={'acc': 0.8325, 'prec': 0.6882}]\n",
      "valid: 100%|██████████| 102/102 [00:00<00:00, 170.07it/s, loss=0.35, metrics={'acc': 0.8319, 'prec': 0.687}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try more models.  As you can see, you can run a wide and deep model in just a few lines of code. The simple MLP approach provided a logloss of 0.349.  \n",
    "  \n",
    "Let's now walk through some more advanced approaches starting with the `FT-Transformer` as the `deeptabular` component. This model uses a simple transformer approach for tabular data explained at [https://arxiv.org/abs/2106.11959](https://arxiv.org/abs/2106.11959). It does require slightly different preprocessing and combines a deep approach with a deep tabular approach."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\n",
    "cat_embed_cols_for_transformer = [el[0] for el in cat_embed_cols]\n",
    "tab_preprocessor = TabPreprocessor(embed_cols=cat_embed_cols_for_transformer, \n",
    "                                   continuous_cols=continuous_cols, \n",
    "                                   for_transformer=True, \n",
    "                                   with_cls_token = True)\n",
    "X_tab = tab_preprocessor.fit_transform(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "wide = Wide(wide_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "deeptabular = TabTransformer(column_idx=tab_preprocessor.column_idx,\n",
    "                             embed_input=tab_preprocessor.embeddings_input,\n",
    "                             continuous_cols=continuous_cols, \n",
    "                             embed_continuous = True, \n",
    "                             embed_continuous_activation = \"relu\")\n",
    "model = WideDeep(wide=wide, deeptabular=deeptabular)\n",
    "tabt_model = Trainer(model, objective='binary', metrics=[Accuracy, Precision])\n",
    "tabt_model.fit(X_wide=X_wide, X_tab=X_tab, target=target, n_epochs=5, batch_size=512, val_split=0.2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/rajiv.shah/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "epoch 1: 100%|██████████| 51/51 [00:05<00:00,  9.29it/s, loss=0.558, metrics={'acc': 0.7252, 'prec': 0.4082}]\n",
      "valid: 100%|██████████| 13/13 [00:00<00:00, 29.57it/s, loss=0.435, metrics={'acc': 0.7918, 'prec': 0.6035}]\n",
      "epoch 2: 100%|██████████| 51/51 [00:05<00:00,  8.85it/s, loss=0.415, metrics={'acc': 0.8058, 'prec': 0.6265}]\n",
      "valid: 100%|██████████| 13/13 [00:00<00:00, 25.54it/s, loss=0.38, metrics={'acc': 0.8268, 'prec': 0.703}]\n",
      "epoch 3: 100%|██████████| 51/51 [00:06<00:00,  8.33it/s, loss=0.388, metrics={'acc': 0.8206, 'prec': 0.6537}]\n",
      "valid: 100%|██████████| 13/13 [00:00<00:00, 29.39it/s, loss=0.376, metrics={'acc': 0.822, 'prec': 0.6582}]\n",
      "epoch 4: 100%|██████████| 51/51 [00:05<00:00,  8.95it/s, loss=0.382, metrics={'acc': 0.8213, 'prec': 0.6556}]\n",
      "valid: 100%|██████████| 13/13 [00:00<00:00, 29.12it/s, loss=0.382, metrics={'acc': 0.8283, 'prec': 0.7446}]\n",
      "epoch 5: 100%|██████████| 51/51 [00:05<00:00,  8.98it/s, loss=0.378, metrics={'acc': 0.8258, 'prec': 0.6634}]\n",
      "valid: 100%|██████████| 13/13 [00:00<00:00, 29.14it/s, loss=0.362, metrics={'acc': 0.8353, 'prec': 0.6882}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now try a few of the cutting edge models, such as `Perceiver` and `TabFastFormer`.  [DeepMind's Perceiver](https://arxiv.org/pdf/2103.03206.pdf) is built for handling multimodel data.  `TabFastFormer` is an adaptation of [Fastformer: Additive Attention Can Be All You Need](https://arxiv.org/pdf/2108.09084.pdf). As you will see, Fastformer gives the lowest loss."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "tabperceiver = TabPerceiver(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    embed_input=tab_preprocessor.embeddings_input,\n",
    "    continuous_cols=tab_preprocessor.continuous_cols, \n",
    "    n_perceiver_blocks=1, \n",
    "    n_latent_blocks=3, \n",
    "    n_latent_heads=2, \n",
    "    n_latents=6,\n",
    "    latent_dim=32,\n",
    ")\n",
    "model = WideDeep(deeptabular=tabperceiver)\n",
    "perc_model = Trainer(model, objective='binary', metrics=[Accuracy])\n",
    "perc_model.fit(X_tab=X_tab, target=target, n_epochs=5, batch_size=256, val_split=0.2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/rajiv.shah/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "epoch 1: 100%|██████████| 102/102 [00:05<00:00, 18.78it/s, loss=0.447, metrics={'acc': 0.792}]\n",
      "valid: 100%|██████████| 26/26 [00:00<00:00, 50.60it/s, loss=0.365, metrics={'acc': 0.8282}]\n",
      "epoch 2: 100%|██████████| 102/102 [00:05<00:00, 18.31it/s, loss=0.369, metrics={'acc': 0.8281}]\n",
      "valid: 100%|██████████| 26/26 [00:00<00:00, 46.34it/s, loss=0.356, metrics={'acc': 0.8323}]\n",
      "epoch 3: 100%|██████████| 102/102 [00:06<00:00, 17.00it/s, loss=0.359, metrics={'acc': 0.8327}]\n",
      "valid: 100%|██████████| 26/26 [00:00<00:00, 48.79it/s, loss=0.356, metrics={'acc': 0.8314}]\n",
      "epoch 4: 100%|██████████| 102/102 [00:06<00:00, 16.29it/s, loss=0.359, metrics={'acc': 0.835}]\n",
      "valid: 100%|██████████| 26/26 [00:00<00:00, 44.69it/s, loss=0.352, metrics={'acc': 0.8357}]\n",
      "epoch 5: 100%|██████████| 102/102 [00:06<00:00, 16.34it/s, loss=0.355, metrics={'acc': 0.8371}]\n",
      "valid: 100%|██████████| 26/26 [00:00<00:00, 42.92it/s, loss=0.347, metrics={'acc': 0.8382}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "tabfastformer = TabFastFormer(column_idx=tab_preprocessor.column_idx,\n",
    "              embed_input=tab_preprocessor.embeddings_input,\n",
    "              continuous_cols=tab_preprocessor.continuous_cols, \n",
    "              n_blocks=2, n_heads=4,             \n",
    "            )\n",
    "model = WideDeep(deeptabular=tabfastformer)\n",
    "fasttab_model = Trainer(model, objective='binary', metrics=[Accuracy])\n",
    "fasttab_model.fit(X_tab=X_tab, target=target, n_epochs=5, batch_size=256, val_split=0.2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/rajiv.shah/anaconda3/envs/pytorch/lib/python3.7/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "epoch 1: 100%|██████████| 102/102 [00:03<00:00, 27.53it/s, loss=0.462, metrics={'acc': 0.7812}]\n",
      "valid: 100%|██████████| 26/26 [00:00<00:00, 54.17it/s, loss=0.376, metrics={'acc': 0.8079}]\n",
      "epoch 2: 100%|██████████| 102/102 [00:03<00:00, 26.53it/s, loss=0.375, metrics={'acc': 0.8228}]\n",
      "valid: 100%|██████████| 26/26 [00:00<00:00, 63.96it/s, loss=0.352, metrics={'acc': 0.836}]\n",
      "epoch 3: 100%|██████████| 102/102 [00:03<00:00, 27.64it/s, loss=0.363, metrics={'acc': 0.8323}]\n",
      "valid: 100%|██████████| 26/26 [00:00<00:00, 59.87it/s, loss=0.353, metrics={'acc': 0.8348}]\n",
      "epoch 4: 100%|██████████| 102/102 [00:03<00:00, 28.09it/s, loss=0.36, metrics={'acc': 0.8325}]\n",
      "valid: 100%|██████████| 26/26 [00:00<00:00, 66.68it/s, loss=0.376, metrics={'acc': 0.827}]\n",
      "epoch 5: 100%|██████████| 102/102 [00:03<00:00, 26.70it/s, loss=0.354, metrics={'acc': 0.8348}]\n",
      "valid: 100%|██████████| 26/26 [00:00<00:00, 64.24it/s, loss=0.344, metrics={'acc': 0.8369}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explainability\n",
    "\n",
    "Explainability is very important for understanding how your model is working and getting others to trust your model.  All of these models will work with traditional model agnostic explainablity techniques like permutation based feature importance, partial dependence, and Shap explanations.  Additionally it's possible to extract our the attention weights to better understand what the model is focused on.  You can find this in the companion notebook location XYZ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\n",
    "feature_perm = FeaturePermutation(model.deeptabular.eval())\n",
    "attr_fic = feature_perm.attribute(torch.from_numpy(X_tab),target=0)  ##Run here against training data\n",
    "attr_fic_sum = torch.div(torch.sum((np.absolute(attr_fic)),0),len(attr_fic))\n",
    "attr_fic_sum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.6454, 1.4507, 0.1671, 0.7164, 0.0971, 0.7971, 0.6541])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Helper method to print importances and visualize feature importance\n",
    "\n",
    "def visualize_importances(feature_names, importances, title=\"Feature Importance for Permutation Importance\", plot=True, axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    print (feature_names)\n",
    "    for i in range(len(feature_names)):\n",
    "        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.barh(x_pos, importances, align='center')\n",
    "        plt.yticks(x_pos, feature_names, wrap=True)\n",
    "        plt.ylabel(axis_title)\n",
    "        plt.title(title)\n",
    "fnl = list(tab_preprocessor.column_idx.keys())\n",
    "visualize_importances(fnl, attr_fic_sum.numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature Importance for Permutation Importance\n",
      "['cls_token', 'education', 'relationship', 'workclass', 'occupation', 'native_country', 'age', 'hours_per_week']\n",
      "cls_token :  0.000\n",
      "education :  0.645\n",
      "relationship :  1.451\n",
      "workclass :  0.167\n",
      "occupation :  0.716\n",
      "native_country :  0.097\n",
      "age :  0.797\n",
      "hours_per_week :  0.654\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predictions and Saving your model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you have trained the model, you can always run predictions against it for your new data.  Here is a simple command for getting probabilities back from a classification model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "results = fasttab_model.predict_proba(X_tab=X_tab)\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "predict: 100%|██████████| 128/128 [00:01<00:00, 90.72it/s] \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[9.68174100e-01, 3.18259038e-02],\n",
       "       [4.96614397e-01, 5.03385603e-01],\n",
       "       [9.90554571e-01, 9.44540463e-03],\n",
       "       ...,\n",
       "       [9.88520682e-01, 1.14793219e-02],\n",
       "       [9.99272227e-01, 7.27771898e-04],\n",
       "       [2.80827999e-01, 7.19172001e-01]])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are some commands for saving preprocessing steps, the model, and training history.  There are example notebooks that cover [saving and loading a model](https://github.com/jrzaurin/pytorch-widedeep/blob/master/examples/08_save_and_load_model_and_artifacts.ipynb)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "import pickle\n",
    "\n",
    "#save preprocessing\n",
    "with open('tab_preproc.pkl', 'wb') as dp:\n",
    "    pickle.dump(tab_preprocessor, dp)\n",
    "\n",
    "#save the model and training history\n",
    "fasttab_model.save(path =\"\",model_filename=\"model_saved.pt\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "interpreter": {
   "hash": "a03b9aa3c84f755c4ab71ec9dc21fa9ce0f88d663de0825ed331c1e1e0b364ef"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}